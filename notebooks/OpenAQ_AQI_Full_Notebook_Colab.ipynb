{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# OpenAQ AQI Prediction \u2014 Full Colab Notebook\n", "\n", "End-to-end pipeline:\n", "- Load `openaq.csv` from Google Drive\n", "- Use `preprocessing.py` from the cloned GitHub repo to clean/standardize\n", "- Pivot long \u2192 wide (pollutants as columns)\n", "- Compute AQI (US EPA breakpoints) from standardized pollutant concentrations\n", "- Build time + lag features\n", "- Train baseline + RandomForest with time-series CV\n", "\n", "**Expected Drive path:** `/content/drive/MyDrive/data_set/openaq.csv`  \n", "**Outputs saved to:** `/content/drive/MyDrive/data/`"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 0) Setup: Drive + Repo\n", "# =============================\n", "from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "# Clone repo (safe if already cloned)\n", "!test -d /content/openaq-aqi-predictor || git clone https://github.com/AshVenn/openaq-aqi-predictor.git\n", "\n", "from pathlib import Path\n", "import sys\n", "\n", "REPO_ROOT = Path(\"/content/openaq-aqi-predictor\").resolve()\n", "SRC_DIR = REPO_ROOT / \"src\"\n", "sys.path.insert(0, str(SRC_DIR))\n", "\n", "# Paths\n", "DATA_PATH = Path(\"/content/drive/MyDrive/data_set/openaq.csv\").resolve()\n", "OUTPUT_DIR = Path(\"/content/drive/MyDrive/data\").resolve()\n", "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n", "\n", "print(\"Repo:\", REPO_ROOT.exists(), \"| src:\", SRC_DIR.exists())\n", "print(\"Dataset:\", DATA_PATH.exists())\n", "print(\"Output dir:\", OUTPUT_DIR)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 1) Imports\n", "# =============================\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "SEED = 42\n", "np.random.seed(SEED)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 2) Load + clean (from repo preprocessing.py)\n", "# =============================\n", "import importlib\n", "import preprocessing\n", "importlib.reload(preprocessing)\n", "\n", "from preprocessing import load_raw_data, clean_raw_data\n", "\n", "raw_df = load_raw_data(str(DATA_PATH))\n", "clean_df = clean_raw_data(raw_df)\n", "\n", "print(\"raw_df:\", raw_df.shape)\n", "print(\"clean_df:\", clean_df.shape)\n", "display(clean_df.head())\n", "print(\"Columns:\", list(clean_df.columns))\n", "\n", "# Save long cleaned data\n", "processed_long_path = OUTPUT_DIR / \"processed_long.csv\"\n", "clean_df.to_csv(processed_long_path, index=False)\n", "print(\"Saved:\", processed_long_path)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 3) Quick sanity plots (long format)\n", "# =============================\n", "clean_df[\"timestamp\"] = pd.to_datetime(clean_df[\"timestamp\"], errors=\"coerce\")\n", "\n", "counts = clean_df.groupby(clean_df[\"timestamp\"].dt.date).size()\n", "plt.figure(figsize=(10,4))\n", "counts.plot()\n", "plt.title(\"Daily Measurement Counts\")\n", "plt.xlabel(\"Date\")\n", "plt.ylabel(\"Count\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Pollutant distribution (counts)\n", "plt.figure(figsize=(8,4))\n", "clean_df[\"pollutant\"].value_counts().head(15).plot(kind=\"bar\")\n", "plt.title(\"Top pollutants by count\")\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## AQI computation\n", "We compute AQI from pollutant concentrations using **US EPA** breakpoints.\n", "\n", "- Compute **sub-index (IAQI)** for each pollutant using linear interpolation\n", "- Final **AQI = max(IAQI)** across pollutants\n", "\n", "This notebook assumes `value_std` is already standardized by your `clean_raw_data()` step.\n", "If a pollutant is missing at a timestamp/location, it\u2019s ignored for the max."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 4) Pivot long -> wide (ML-ready table)\n", "# Your clean_df columns: ['city','location','latitude','longitude','timestamp','pollutant','value_std','unit_std','source_name']\n", "# =============================\n", "df = clean_df.copy()\n", "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n", "df[\"pollutant\"] = df[\"pollutant\"].astype(str).str.lower()\n", "\n", "wanted_pollutants = [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\"]\n", "df = df[df[\"pollutant\"].isin(wanted_pollutants)].copy()\n", "\n", "features_df = (\n", "    df.pivot_table(\n", "        index=[\"city\", \"location\", \"latitude\", \"longitude\", \"timestamp\"],\n", "        columns=\"pollutant\",\n", "        values=\"value_std\",          # IMPORTANT: standardized numeric value\n", "        aggfunc=\"mean\"\n", "    )\n", "    .reset_index()\n", ")\n", "features_df.columns.name = None\n", "\n", "print(\"features_df:\", features_df.shape)\n", "display(features_df.head())\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 5) AQI breakpoints (US EPA) + helpers\n", "# Units (typical US EPA AQI):\n", "# - PM2.5: \u00b5g/m\u00b3 (24h) [we treat your value_std as matching]\n", "# - PM10 : \u00b5g/m\u00b3 (24h)\n", "# - O3   : ppb (8h) or ppm; here use ppb-style breakpoints for simplicity\n", "# - NO2  : ppb (1h)\n", "# - SO2  : ppb (1h)\n", "# - CO   : ppm (8h)\n", "#\n", "# NOTE: If your 'value_std' is in different units, you must convert accordingly.\n", "# We'll include simple conversions for CO if it seems too large/small (optional).\n", "# =============================\n", "from typing import Dict, List, Tuple, Optional\n", "\n", "AQI_BANDS = [\n", "    (0, 50),\n", "    (51, 100),\n", "    (101, 150),\n", "    (151, 200),\n", "    (201, 300),\n", "    (301, 400),\n", "    (401, 500),\n", "]\n", "\n", "# Breakpoints: pollutant -> list of (BPlo, BPhi, Ilo, Ihi)\n", "# These are commonly used US EPA breakpoint ranges.\n", "BREAKPOINTS: Dict[str, List[Tuple[float, float, int, int]]] = {\n", "    \"pm25\": [\n", "        (0.0, 12.0, 0, 50),\n", "        (12.1, 35.4, 51, 100),\n", "        (35.5, 55.4, 101, 150),\n", "        (55.5, 150.4, 151, 200),\n", "        (150.5, 250.4, 201, 300),\n", "        (250.5, 350.4, 301, 400),\n", "        (350.5, 500.4, 401, 500),\n", "    ],\n", "    \"pm10\": [\n", "        (0, 54, 0, 50),\n", "        (55, 154, 51, 100),\n", "        (155, 254, 101, 150),\n", "        (255, 354, 151, 200),\n", "        (355, 424, 201, 300),\n", "        (425, 504, 301, 400),\n", "        (505, 604, 401, 500),\n", "    ],\n", "    # O3 8-hour (ppm) breakpoints often used; here we store in ppb for convenience:\n", "    # 0.054 ppm = 54 ppb, etc.\n", "    \"o3\": [\n", "        (0, 54, 0, 50),\n", "        (55, 70, 51, 100),\n", "        (71, 85, 101, 150),\n", "        (86, 105, 151, 200),\n", "        (106, 200, 201, 300),\n", "    ],\n", "    \"no2\": [\n", "        (0, 53, 0, 50),\n", "        (54, 100, 51, 100),\n", "        (101, 360, 101, 150),\n", "        (361, 649, 151, 200),\n", "        (650, 1249, 201, 300),\n", "        (1250, 1649, 301, 400),\n", "        (1650, 2049, 401, 500),\n", "    ],\n", "    \"so2\": [\n", "        (0, 35, 0, 50),\n", "        (36, 75, 51, 100),\n", "        (76, 185, 101, 150),\n", "        (186, 304, 151, 200),\n", "        (305, 604, 201, 300),\n", "        (605, 804, 301, 400),\n", "        (805, 1004, 401, 500),\n", "    ],\n", "    # CO in ppm (8h)\n", "    \"co\": [\n", "        (0.0, 4.4, 0, 50),\n", "        (4.5, 9.4, 51, 100),\n", "        (9.5, 12.4, 101, 150),\n", "        (12.5, 15.4, 151, 200),\n", "        (15.5, 30.4, 201, 300),\n", "        (30.5, 40.4, 301, 400),\n", "        (40.5, 50.4, 401, 500),\n", "    ],\n", "}\n", "\n", "def iaqi_from_breakpoints(c: float, bps: List[Tuple[float, float, int, int]]) -> Optional[float]:\n", "    if c is None or (isinstance(c, float) and np.isnan(c)):\n", "        return None\n", "    for BPlo, BPhi, Ilo, Ihi in bps:\n", "        if BPlo <= c <= BPhi:\n", "            # IAQI = (Ihi \u2212 Ilo)/(BPhi \u2212 BPlo) \u00d7 (C \u2212 BPlo) + Ilo\n", "            return (Ihi - Ilo) / (BPhi - BPlo) * (c - BPlo) + Ilo\n", "    # Out of range: cap to 500 if above highest\n", "    if c > bps[-1][1]:\n", "        return 500.0\n", "    return None\n", "\n", "def compute_aqi_row(row: pd.Series, pollutants: List[str]) -> float:\n", "    iaqis = []\n", "    for p in pollutants:\n", "        if p not in BREAKPOINTS or p not in row.index:\n", "            continue\n", "        val = row[p]\n", "        # Optional heuristic for O3: if values look like ppm (e.g., 0.07), convert to ppb\n", "        if p == \"o3\" and pd.notna(val) and val < 1.0:\n", "            val = val * 1000.0  # ppm -> ppb heuristic\n", "        # Optional heuristic for NO2/SO2: if values look like ppm (<1), convert to ppb\n", "        if p in (\"no2\",\"so2\") and pd.notna(val) and val < 1.0:\n", "            val = val * 1000.0\n", "        iaqi = iaqi_from_breakpoints(float(val), BREAKPOINTS[p])\n", "        if iaqi is not None:\n", "            iaqis.append(iaqi)\n", "    return float(np.max(iaqis)) if iaqis else np.nan\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 6) Compute AQI target\n", "# =============================\n", "pollutant_cols_present = [c for c in [\"pm25\",\"pm10\",\"no2\",\"o3\",\"co\",\"so2\"] if c in features_df.columns]\n", "print(\"Pollutant cols present:\", pollutant_cols_present)\n", "\n", "features_df[\"aqi\"] = features_df.apply(lambda r: compute_aqi_row(r, pollutant_cols_present), axis=1)\n", "\n", "print(\"AQI null rate:\", features_df[\"aqi\"].isna().mean())\n", "display(features_df[[\"timestamp\",\"location\",\"aqi\"] + pollutant_cols_present].head())\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 7) Add time + lag features (from repo features.py)\n", "# =============================\n", "from features import add_time_features, add_lag_features, build_feature_columns\n", "\n", "features_df = add_time_features(features_df, time_col=\"timestamp\")\n", "\n", "features_df = add_lag_features(\n", "    features_df,\n", "    group_cols=[\"location\"],\n", "    target_cols=pollutant_cols_present,\n", "    lags=(1,),\n", "    time_col=\"timestamp\"\n", ")\n", "\n", "feature_cols = build_feature_columns(pollutant_cols_present, include_lags=True)\n", "\n", "print(\"Num feature cols:\", len(feature_cols))\n", "print(feature_cols[:10], \"...\")\n", "\n", "# Save engineered dataset\n", "processed_features_path = OUTPUT_DIR / \"processed_features.csv\"\n", "features_df.to_csv(processed_features_path, index=False)\n", "print(\"Saved:\", processed_features_path)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 8) Quick EDA: AQI distribution + correlation\n", "# =============================\n", "plt.figure(figsize=(8,4))\n", "features_df[\"aqi\"].dropna().hist(bins=50)\n", "plt.title(\"AQI distribution\")\n", "plt.xlabel(\"AQI\")\n", "plt.ylabel(\"Frequency\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Correlation heatmap (numeric only)\n", "num_df = features_df[[c for c in (pollutant_cols_present + [\"aqi\",\"hour\",\"day_of_week\",\"month\",\"latitude\",\"longitude\"]) if c in features_df.columns]].dropna()\n", "corr = num_df.corr(numeric_only=True)\n", "\n", "plt.figure(figsize=(8,6))\n", "plt.imshow(corr.values)\n", "plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\n", "plt.yticks(range(len(corr.columns)), corr.columns)\n", "plt.colorbar()\n", "plt.title(\"Correlation heatmap\")\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 9) Modeling (baseline + RandomForest)\n", "# =============================\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n", "\n", "def train_test_split_time(df, time_col=\"timestamp\", test_size=0.2):\n", "    df = df.sort_values(time_col)\n", "    split_index = int(len(df) * (1 - test_size))\n", "    return df.iloc[:split_index].copy(), df.iloc[split_index:].copy()\n", "\n", "def regression_metrics(y_true, y_pred):\n", "    mse = mean_squared_error(y_true, y_pred)  # compatible across sklearn versions\n", "    rmse = float(np.sqrt(mse))\n", "    return {\n", "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n", "        \"rmse\": rmse,\n", "        \"r2\": float(r2_score(y_true, y_pred)),\n", "    }\n", "\n", "def summarize_metrics(m):\n", "    return f\"MAE={m['mae']:.2f}, RMSE={m['rmse']:.2f}, R2={m['r2']:.3f}\"\n", "\n", "df_model = features_df.dropna(subset=[\"aqi\"]).copy()\n", "\n", "# Keep only existing feature cols\n", "feature_cols_final = [c for c in feature_cols if c in df_model.columns]\n", "\n", "train_df, test_df = train_test_split_time(df_model, time_col=\"timestamp\", test_size=0.2)\n", "\n", "X_train = train_df[feature_cols_final].values\n", "y_train = train_df[\"aqi\"].values\n", "X_test  = test_df[feature_cols_final].values\n", "y_test  = test_df[\"aqi\"].values\n", "\n", "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n", "print(\"Num features:\", len(feature_cols_final))\n", "\n", "baseline_model = Pipeline([\n", "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n", "    (\"model\", LinearRegression())\n", "])\n", "baseline_model.fit(X_train, y_train)\n", "baseline_pred = baseline_model.predict(X_test)\n", "baseline_m = regression_metrics(y_test, baseline_pred)\n", "print(\"Baseline:\", summarize_metrics(baseline_m))\n", "\n", "param_grid = {\n", "    \"model__n_estimators\": [200, 400],\n", "    \"model__max_depth\": [None, 10, 20],\n", "    \"model__min_samples_split\": [2, 5],\n", "    \"model__min_samples_leaf\": [1, 2],\n", "}\n", "rf_pipe = Pipeline([\n", "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n", "    (\"model\", RandomForestRegressor(random_state=SEED, n_jobs=-1))\n", "])\n", "\n", "grid = GridSearchCV(\n", "    rf_pipe,\n", "    param_grid=param_grid,\n", "    cv=TimeSeriesSplit(n_splits=3),\n", "    scoring=\"neg_mean_absolute_error\",\n", "    n_jobs=-1\n", ")\n", "grid.fit(X_train, y_train)\n", "\n", "best_model = grid.best_estimator_\n", "pred = best_model.predict(X_test)\n", "tree_m = regression_metrics(y_test, pred)\n", "\n", "print(\"Tree:\", summarize_metrics(tree_m))\n", "print(\"Best params:\", grid.best_params_)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# =============================\n", "# 10) Diagnostics\n", "# =============================\n", "errors = np.abs(y_test - pred)\n", "print(\"Median abs error:\", float(np.median(errors)))\n", "print(\"90th pct abs error:\", float(np.percentile(errors, 90)))\n", "print(\"99th pct abs error:\", float(np.percentile(errors, 99)))\n", "\n", "# Predicted vs Actual\n", "plt.figure(figsize=(6,6))\n", "plt.scatter(y_test, pred, alpha=0.4)\n", "mn = float(min(y_test.min(), pred.min()))\n", "mx = float(max(y_test.max(), pred.max()))\n", "plt.plot([mn, mx], [mn, mx], \"r--\")\n", "plt.xlabel(\"Actual AQI\")\n", "plt.ylabel(\"Predicted AQI\")\n", "plt.title(\"Predicted vs Actual AQI\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Error histogram\n", "plt.figure(figsize=(8,4))\n", "plt.hist(errors, bins=50)\n", "plt.title(\"Absolute error distribution\")\n", "plt.xlabel(\"|error|\")\n", "plt.ylabel(\"count\")\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "# Feature importance\n", "model = best_model.named_steps[\"model\"]\n", "if hasattr(model, \"feature_importances_\"):\n", "    importances = model.feature_importances_\n", "    order = np.argsort(importances)[::-1]\n", "    plt.figure(figsize=(10,4))\n", "    plt.bar(range(len(feature_cols_final)), importances[order])\n", "    plt.xticks(range(len(feature_cols_final)), np.array(feature_cols_final)[order], rotation=45, ha=\"right\")\n", "    plt.title(\"Feature importance (RandomForest)\")\n", "    plt.tight_layout()\n", "    plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Notes / Next steps\n", "- If AQI spikes dominate RMSE: try predicting `log1p(AQI)` and invert.\n", "- Add rolling features (3h/6h) per location to improve temporal signal.\n", "- Consider filtering rows with too many missing pollutant measurements.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}