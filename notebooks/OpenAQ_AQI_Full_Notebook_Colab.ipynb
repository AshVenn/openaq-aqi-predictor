{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d8d75042",
      "metadata": {},
      "source": [
        "# AQI Estimation Model (Colab)\n",
        "\n",
        "This notebook builds a full AQI estimation pipeline:\n",
        "1. Clone repo + setup\n",
        "2. Load & preprocess raw OpenAQ-like data\n",
        "3. Aggregate/pivot to wide format + compute deterministic AQI labels\n",
        "4. Feature engineering for estimation (no leakage)\n",
        "5. Safe missingness simulation\n",
        "6. Train/compare models, select best, export artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730703d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Clone repo (Colab) ===\n",
        "!git clone https://github.com/AshVenn/openaq-aqi-predictor.git\n",
        "%cd openaq-aqi-predictor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e468a13e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Install deps (Colab) ===\n",
        "!pip -q install xgboost catboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7334f602",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Setup ===\n",
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from src import preprocessing\n",
        "from src.aqi import compute_aqi_dataframe\n",
        "from src.evaluate import regression_metrics\n",
        "from src.features import add_time_features\n",
        "\n",
        "POLLUTANTS_ALL = [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\"]\n",
        "INPUT_POLLUTANTS = POLLUTANTS_ALL.copy()\n",
        "SIMULATE_MISSINGNESS = True\n",
        "MISSING_PROB = 0.2\n",
        "MIN_NON_MISSING_PER_COL = 50\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_FREQ = \"D\"\n",
        "TEST_SIZE = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149162a4",
      "metadata": {},
      "source": [
        "## 1) Load & preprocess raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73421a24",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_path = ROOT / \"data\" / \"openaq.csv\"\n",
        "\n",
        "raw_df = preprocessing.load_raw_data(str(raw_path))\n",
        "clean_df = preprocessing.clean_raw_data(raw_df)\n",
        "\n",
        "print(f\"Raw rows: {len(raw_df):,}\")\n",
        "print(f\"Clean rows: {len(clean_df):,}\")\n",
        "\n",
        "# Debug checks for pm25 presence and conversion\n",
        "if \"pollutant\" in clean_df.columns:\n",
        "    pm25_rows = clean_df[clean_df[\"pollutant\"] == \"pm25\"]\n",
        "    print(f\"pm25 rows in cleaned long data: {len(pm25_rows):,}\")\n",
        "    if \"value_std\" in pm25_rows.columns:\n",
        "        print(f\"pm25 non-null value_std: {pm25_rows['value_std'].notna().sum():,}\")\n",
        "else:\n",
        "    print(\"No 'pollutant' column found in clean_df\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94b79e6",
      "metadata": {},
      "source": [
        "## 2) Aggregate/pivot + compute deterministic AQI label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5b0e85",
      "metadata": {},
      "outputs": [],
      "source": [
        "wide_df = preprocessing.aggregate_and_pivot(clean_df, freq=TRAIN_FREQ)\n",
        "\n",
        "# Compute deterministic AQI label (US EPA breakpoints)\n",
        "aqi_df = compute_aqi_dataframe(wide_df)\n",
        "\n",
        "# Persist processed datasets\n",
        "processed_wide_path = ROOT / \"data\" / \"processed_wide.csv\"\n",
        "processed_aqi_path = ROOT / \"data\" / \"processed_aqi.csv\"\n",
        "wide_df.to_csv(processed_wide_path, index=False)\n",
        "aqi_df.to_csv(processed_aqi_path, index=False)\n",
        "\n",
        "print(f\"Saved: {processed_wide_path}\")\n",
        "print(f\"Saved: {processed_aqi_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c203d0",
      "metadata": {},
      "source": [
        "## 3) Feature engineering for AQI estimation (no leakage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453883e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_pollutant_columns(df, pollutant_cols):\n",
        "    df = df.copy()\n",
        "    for p in pollutant_cols:\n",
        "        if p not in df.columns:\n",
        "            df[p] = np.nan\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_missingness_indicators(df, pollutant_cols):\n",
        "    df = df.copy()\n",
        "    for p in pollutant_cols:\n",
        "        df[f\"{p}_is_missing\"] = df[p].isna().astype(int)\n",
        "    return df\n",
        "\n",
        "\n",
        "features_df = ensure_pollutant_columns(aqi_df, POLLUTANTS_ALL)\n",
        "features_df = add_time_features(features_df, time_col=\"timestamp\")\n",
        "\n",
        "# Drop rows without AQI labels\n",
        "features_df = features_df[features_df[\"aqi\"].notna()].copy()\n",
        "\n",
        "# Determine available pollutants (non-null > 0)\n",
        "available_pollutants = [\n",
        "    p for p in POLLUTANTS_ALL\n",
        "    if p in features_df.columns and features_df[p].notna().sum() > 0\n",
        "]\n",
        "\n",
        "print(\"Non-null counts per pollutant:\")\n",
        "for p in POLLUTANTS_ALL:\n",
        "    count = features_df[p].notna().sum() if p in features_df.columns else 0\n",
        "    print(f\"{p}: {count}\")\n",
        "\n",
        "INPUT_POLLUTANTS = available_pollutants\n",
        "print(f\"Using INPUT_POLLUTANTS: {INPUT_POLLUTANTS}\")\n",
        "\n",
        "feature_cols = (\n",
        "    [\"latitude\", \"longitude\", \"hour\", \"day_of_week\", \"month\"]\n",
        "    + INPUT_POLLUTANTS\n",
        "    + [f\"{p}_is_missing\" for p in INPUT_POLLUTANTS]\n",
        ")\n",
        "\n",
        "assert \"aqi\" not in feature_cols\n",
        "print(\"Feature columns:\")\n",
        "print(feature_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "478c9a4d",
      "metadata": {},
      "source": [
        "## 4) Safe missingness simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c376c5b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n",
        "def simulate_missingness(\n",
        "    df,\n",
        "    pollutant_cols,\n",
        "    missing_prob=0.2,\n",
        "    min_non_missing_per_col=50,\n",
        "    seed=42,\n",
        "):\n",
        "    df = df.copy()\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    original = df[pollutant_cols].copy()\n",
        "\n",
        "    # Apply random missingness per cell (only where value exists)\n",
        "    mask = rng.rand(*original.shape) < missing_prob\n",
        "    mask = mask & original.notna().values\n",
        "    df[pollutant_cols] = original.mask(mask)\n",
        "\n",
        "    # Row rule: ensure at least one pollutant remains per row if any were present\n",
        "    all_missing = df[pollutant_cols].isna().all(axis=1)\n",
        "    for idx in df.index[all_missing]:\n",
        "        available = original.loc[idx].dropna()\n",
        "        if not available.empty:\n",
        "            restore_col = rng.choice(available.index)\n",
        "            df.at[idx, restore_col] = original.at[idx, restore_col]\n",
        "\n",
        "    # Column rule: ensure minimum non-missing per pollutant (global)\n",
        "    for p in pollutant_cols:\n",
        "        current_non_missing = df[p].notna().sum()\n",
        "        if current_non_missing >= min_non_missing_per_col:\n",
        "            continue\n",
        "        candidates = original[p][original[p].notna()].index\n",
        "        if len(candidates) == 0:\n",
        "            continue\n",
        "        needed = min_non_missing_per_col - current_non_missing\n",
        "        restore_rows = rng.choice(candidates, size=min(needed, len(candidates)), replace=False)\n",
        "        df.loc[restore_rows, p] = original.loc[restore_rows, p]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def enforce_non_missing_per_fold(df, original, pollutant_cols, n_splits=3, seed=42):\n",
        "    df = df.copy()\n",
        "    rng = np.random.RandomState(seed)\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    indices = np.arange(len(df))\n",
        "\n",
        "    for train_idx, _ in tscv.split(indices):\n",
        "        for p in pollutant_cols:\n",
        "            if df.iloc[train_idx][p].notna().sum() == 0:\n",
        "                candidates = train_idx[original.iloc[train_idx][p].notna().values]\n",
        "                if len(candidates) == 0:\n",
        "                    continue\n",
        "                restore_pos = rng.choice(candidates, size=1, replace=False)\n",
        "                df.iloc[restore_pos, df.columns.get_loc(p)] = original.iloc[restore_pos][p].values\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "original_pollutants = features_df[INPUT_POLLUTANTS].copy()\n",
        "\n",
        "if SIMULATE_MISSINGNESS and INPUT_POLLUTANTS:\n",
        "    features_df = simulate_missingness(\n",
        "        features_df,\n",
        "        INPUT_POLLUTANTS,\n",
        "        missing_prob=MISSING_PROB,\n",
        "        min_non_missing_per_col=MIN_NON_MISSING_PER_COL,\n",
        "        seed=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    # Ensure no fold has a fully-missing pollutant column\n",
        "    features_df = enforce_non_missing_per_fold(\n",
        "        features_df,\n",
        "        original_pollutants,\n",
        "        INPUT_POLLUTANTS,\n",
        "        n_splits=3,\n",
        "        seed=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "# Missingness indicators AFTER simulation\n",
        "features_df = add_missingness_indicators(features_df, INPUT_POLLUTANTS)\n",
        "\n",
        "# Sanity: no pollutant column is fully missing\n",
        "for p in INPUT_POLLUTANTS:\n",
        "    assert features_df[p].notna().sum() > 0, f\"{p} is fully missing\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0539520f",
      "metadata": {},
      "source": [
        "## 5) Train, tune, compare models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66538f5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_split(df, time_col=\"timestamp\", test_size=0.2):\n",
        "    df = df.sort_values(time_col)\n",
        "    split_idx = int(len(df) * (1 - test_size))\n",
        "    return df.iloc[:split_idx].copy(), df.iloc[split_idx:].copy()\n",
        "\n",
        "\n",
        "def build_pipeline(estimator):\n",
        "    return Pipeline(\n",
        "        steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"model\", estimator),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "train_df, test_df = time_split(features_df, test_size=TEST_SIZE)\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[\"aqi\"]\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df[\"aqi\"]\n",
        "\n",
        "print(\"Non-missing counts in TRAIN split:\")\n",
        "print(X_train[INPUT_POLLUTANTS].notna().sum())\n",
        "\n",
        "model_specs = {\n",
        "    \"LinearRegression\": {\n",
        "        \"estimator\": LinearRegression(),\n",
        "        \"param_grid\": None,\n",
        "    },\n",
        "    \"Ridge\": {\n",
        "        \"estimator\": Ridge(random_state=RANDOM_SEED),\n",
        "        \"param_grid\": {\"model__alpha\": [0.1, 1.0, 10.0, 100.0]},\n",
        "    },\n",
        "    \"Lasso\": {\n",
        "        \"estimator\": Lasso(random_state=RANDOM_SEED, max_iter=5000),\n",
        "        \"param_grid\": {\"model__alpha\": [0.001, 0.01, 0.1, 1.0]},\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        \"estimator\": RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1),\n",
        "        \"param_grid\": {\n",
        "            \"model__n_estimators\": [200, 400],\n",
        "            \"model__max_depth\": [None, 10, 20],\n",
        "            \"model__min_samples_split\": [2, 5],\n",
        "            \"model__min_samples_leaf\": [1, 2],\n",
        "        },\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"estimator\": XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            random_state=RANDOM_SEED,\n",
        "            n_jobs=-1,\n",
        "        ),\n",
        "        \"param_grid\": {\n",
        "            \"model__n_estimators\": [300, 600],\n",
        "            \"model__max_depth\": [4, 6, 8],\n",
        "            \"model__learning_rate\": [0.05, 0.1],\n",
        "            \"model__subsample\": [0.8, 1.0],\n",
        "            \"model__colsample_bytree\": [0.8, 1.0],\n",
        "        },\n",
        "    },\n",
        "    \"CatBoost\": {\n",
        "        \"estimator\": CatBoostRegressor(\n",
        "            loss_function=\"RMSE\",\n",
        "            random_seed=RANDOM_SEED,\n",
        "            verbose=False,\n",
        "        ),\n",
        "        \"param_grid\": {\n",
        "            \"model__depth\": [6, 8, 10],\n",
        "            \"model__learning_rate\": [0.03, 0.1],\n",
        "            \"model__iterations\": [500, 1000],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "results = []\n",
        "best_models = {}\n",
        "cv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "for name, spec in model_specs.items():\n",
        "    pipeline = build_pipeline(spec[\"estimator\"])\n",
        "    if spec[\"param_grid\"]:\n",
        "        grid = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grid=spec[\"param_grid\"],\n",
        "            cv=cv,\n",
        "            scoring=\"neg_mean_absolute_error\",\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        grid.fit(X_train, y_train)\n",
        "        best_estimator = grid.best_estimator_\n",
        "        best_params = grid.best_params_\n",
        "    else:\n",
        "        best_estimator = pipeline.fit(X_train, y_train)\n",
        "        best_params = None\n",
        "\n",
        "    preds = best_estimator.predict(X_test)\n",
        "    metrics = regression_metrics(y_test, preds)\n",
        "\n",
        "    results.append(\n",
        "        {\n",
        "            \"model\": name,\n",
        "            \"mae\": metrics[\"mae\"],\n",
        "            \"rmse\": metrics[\"rmse\"],\n",
        "            \"r2\": metrics[\"r2\"],\n",
        "            \"best_params\": best_params,\n",
        "        }\n",
        "    )\n",
        "    best_models[name] = best_estimator\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values([\"mae\", \"rmse\"]).reset_index(drop=True)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e135bc0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_row = results_df.iloc[0]\n",
        "best_model_name = best_row[\"model\"]\n",
        "best_params = best_row[\"best_params\"]\n",
        "\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "print(f\"Best params: {best_params}\")\n",
        "print(f\"Best metrics: MAE={best_row['mae']:.3f}, RMSE={best_row['rmse']:.3f}, R2={best_row['r2']:.3f}\")\n",
        "\n",
        "# Refit best model on full dataset for export\n",
        "best_spec = model_specs[best_model_name]\n",
        "best_pipeline = build_pipeline(best_spec[\"estimator\"])\n",
        "if best_params:\n",
        "    best_pipeline.set_params(**best_params)\n",
        "\n",
        "X_full = features_df[feature_cols]\n",
        "y_full = features_df[\"aqi\"]\n",
        "best_pipeline.fit(X_full, y_full)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d3abf8f",
      "metadata": {},
      "source": [
        "## 6) Export artifacts + Colab downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ff17b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Export artifacts ===\n",
        "models_dir = ROOT / \"models\"\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "model_path = models_dir / \"aqi_estimator.joblib\"\n",
        "feature_cols_path = models_dir / \"feature_cols.json\"\n",
        "model_meta_path = models_dir / \"model_meta.json\"\n",
        "\n",
        "joblib.dump(best_pipeline, model_path)\n",
        "\n",
        "with open(feature_cols_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(feature_cols, f, indent=2)\n",
        "\n",
        "model_meta = {\n",
        "    \"best_model_name\": best_model_name,\n",
        "    \"best_params\": best_params,\n",
        "    \"metrics\": {\n",
        "        \"mae\": float(best_row[\"mae\"]),\n",
        "        \"rmse\": float(best_row[\"rmse\"]),\n",
        "        \"r2\": float(best_row[\"r2\"]),\n",
        "    },\n",
        "    \"input_pollutants\": INPUT_POLLUTANTS,\n",
        "    \"time_features\": [\"hour\", \"day_of_week\", \"month\"],\n",
        "    \"uses_missingness_indicators\": True,\n",
        "    \"expects_standard_units\": True,\n",
        "}\n",
        "\n",
        "with open(model_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(model_meta, f, indent=2)\n",
        "\n",
        "print(f\"Saved model: {model_path}\")\n",
        "print(f\"Saved feature columns: {feature_cols_path}\")\n",
        "print(f\"Saved metadata: {model_meta_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27044a3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Write report summary ===\n",
        "reports_path = ROOT / \"reports\" / \"summary.md\"\n",
        "\n",
        "summary_lines = [\n",
        "    \"# AQI Estimation Model Summary\",\n",
        "    \"\",\n",
        "    \"## Dataset\",\n",
        "    f\"- Rows after cleaning: {len(clean_df):,}\",\n",
        "    f\"- Rows after aggregation: {len(wide_df):,}\",\n",
        "    f\"- Rows with AQI labels: {len(features_df):,}\",\n",
        "    f\"- Aggregation window: {TRAIN_FREQ}\",\n",
        "    \"\",\n",
        "    \"## Model Comparison (test set)\",\n",
        "    results_df.to_markdown(index=False),\n",
        "    \"\",\n",
        "    \"## Best Model\",\n",
        "    f\"- {best_model_name}\",\n",
        "    f\"- MAE: {best_row['mae']:.2f}\",\n",
        "    f\"- RMSE: {best_row['rmse']:.2f}\",\n",
        "    f\"- R2: {best_row['r2']:.3f}\",\n",
        "]\n",
        "\n",
        "reports_path.write_text(\"\".join(summary_lines), encoding=\"utf-8\")\n",
        "print(f\"Wrote report: {reports_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f035b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Zip + download artifacts in Colab ===\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "zip_path = ROOT / \"aqi_artifacts.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\") as zf:\n",
        "    zf.write(model_path, arcname=f\"models/{model_path.name}\")\n",
        "    zf.write(feature_cols_path, arcname=f\"models/{feature_cols_path.name}\")\n",
        "    zf.write(model_meta_path, arcname=f\"models/{model_meta_path.name}\")\n",
        "    zf.write(reports_path, arcname=\"reports/summary.md\")\n",
        "\n",
        "files.download(str(model_path))\n",
        "files.download(str(feature_cols_path))\n",
        "files.download(str(model_meta_path))\n",
        "files.download(str(reports_path))\n",
        "files.download(str(zip_path))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}