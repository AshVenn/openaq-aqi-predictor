{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Air Quality Modeling - All-in-One Notebook\n",
        "\n",
        "This notebook runs end-to-end in one file with no local module imports.\n",
        "It loads OpenAQ data, cleans it, computes AQI, engineers features, trains models,\n",
        "evaluates performance, and writes a short report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A. Introduction\n",
        "Air quality reflects the concentration of pollutants in the air and their impacts on health.\n",
        "The Air Quality Index (AQI) converts pollutant concentrations into a standardized scale for interpretation.\n",
        "\n",
        "OpenAQ provides raw pollutant concentrations but not AQI because AQI depends on chosen standards,\n",
        "averaging windows, and unit conversions. We compute AQI explicitly in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Data Loading\n",
        "We load the local CSV and inspect schema and samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "ROOT = Path('..').resolve()\n",
        "data_path = ROOT / 'data' / 'openaq.csv'\n",
        "\n",
        "def load_raw_data(path: str) -> pd.DataFrame:\n",
        "    # OpenAQ exports often use semicolon delimiters; allow fallback sniffing.\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
        "        if df.shape[1] <= 1:\n",
        "            raise ValueError('Unexpected delimiter; falling back to sniffing.')\n",
        "    except Exception:\n",
        "        df = pd.read_csv(path, sep=None, engine='python', encoding='utf-8')\n",
        "    return df\n",
        "\n",
        "raw_df = load_raw_data(str(data_path))\n",
        "print(raw_df.shape)\n",
        "display(raw_df.head())\n",
        "display(raw_df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Data Quality Checks\n",
        "We check missing values, duplicates, unit inconsistencies, and outline an outlier detection plan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Missing values\n",
        "missing_pct = raw_df.isna().mean().sort_values(ascending=False)\n",
        "display(missing_pct.to_frame('missing_pct'))\n",
        "\n",
        "# Duplicates\n",
        "dup_count = raw_df.duplicated().sum()\n",
        "print(f'Duplicate rows: {dup_count}')\n",
        "\n",
        "# Unit inconsistencies per pollutant\n",
        "unit_counts = (raw_df[['Pollutant', 'Unit']].dropna().value_counts().reset_index())\n",
        "unit_counts.columns = ['Pollutant', 'Unit', 'count']\n",
        "display(unit_counts.head(20))\n",
        "\n",
        "# Plot: missing value percentages\n",
        "plt.figure(figsize=(8, 4))\n",
        "missing_pct.plot(kind='bar')\n",
        "plt.title('Missing Value Percentage by Column')\n",
        "plt.ylabel('Percentage')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outlier detection plan**\n",
        "- Use robust statistics (IQR/median absolute deviation) per pollutant.\n",
        "- Compare extreme values to AQI breakpoints to flag implausible spikes.\n",
        "- Review outliers per location to distinguish sensor faults vs real events.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Data Engineering: Long ? Wide Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "POLLUTANTS = {'pm25', 'pm10', 'no2', 'o3', 'co', 'so2'}\n",
        "\n",
        "def normalize_unit(unit):\n",
        "    if unit is None:\n",
        "        return None\n",
        "    unit = str(unit).strip().lower()\n",
        "    unit = unit.replace('\\u00b5', 'u')\n",
        "    unit = unit.replace('ug/m^3', 'ug/m3')\n",
        "    unit = unit.replace('ug/m\\u00b3', 'ug/m3')\n",
        "    unit = unit.replace('mg/m\\u00b3', 'mg/m3')\n",
        "    return unit\n",
        "\n",
        "def parse_coordinates(value):\n",
        "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
        "        return None, None\n",
        "    text = str(value)\n",
        "    numbers = re.findall(r'-?\\d+\\.\\d+|-?\\d+', text)\n",
        "    if len(numbers) >= 2:\n",
        "        return float(numbers[0]), float(numbers[1])\n",
        "    return None, None\n",
        "\n",
        "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        norm = col.strip().lower().replace(' ', '_').replace('-', '_')\n",
        "        if norm in ('country', 'city', 'location', 'coordinates', 'pollutant', 'value'):\n",
        "            rename_map[col] = norm\n",
        "        elif norm in ('unit',):\n",
        "            rename_map[col] = 'unit'\n",
        "        elif norm in ('source_name', 'source'):\n",
        "            rename_map[col] = 'source_name'\n",
        "        elif norm in ('last_updated', 'last_updated_utc', 'datetime'):\n",
        "            rename_map[col] = 'last_updated'\n",
        "    return df.rename(columns=rename_map)\n",
        "\n",
        "df = standardize_columns(raw_df)\n",
        "df['pollutant'] = df['pollutant'].astype(str).str.strip().str.lower()\n",
        "df = df[df['pollutant'].isin(POLLUTANTS)]\n",
        "df['unit'] = df['unit'].apply(normalize_unit)\n",
        "df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
        "df['timestamp'] = pd.to_datetime(df['last_updated'], errors='coerce', utc=True).dt.tz_convert(None)\n",
        "\n",
        "lat_lon = df['coordinates'].apply(parse_coordinates)\n",
        "df['latitude'] = lat_lon.apply(lambda x: x[0])\n",
        "df['longitude'] = lat_lon.apply(lambda x: x[1])\n",
        "\n",
        "# Save cleaned long data (without AQI yet)\n",
        "processed_long_path = ROOT / 'data' / 'processed_long.csv'\n",
        "df.to_csv(processed_long_path, index=False)\n",
        "\n",
        "# Aggregate daily to reduce sparsity\n",
        "df['timestamp'] = df['timestamp'].dt.floor('D')\n",
        "group_cols = [c for c in ['country','city','location','latitude','longitude','timestamp','pollutant'] if c in df.columns]\n",
        "grouped = df.groupby(group_cols, dropna=False)['value'].mean().reset_index()\n",
        "\n",
        "wide_df = grouped.pivot_table(\n",
        "    index=[c for c in ['country','city','location','latitude','longitude','timestamp'] if c in grouped.columns],\n",
        "    columns='pollutant',\n",
        "    values='value',\n",
        "    aggfunc='mean',\n",
        ").reset_index()\n",
        "wide_df.columns.name = None\n",
        "\n",
        "processed_wide_path = ROOT / 'data' / 'processed_wide.csv'\n",
        "wide_df.to_csv(processed_wide_path, index=False)\n",
        "\n",
        "display(wide_df.head())\n",
        "\n",
        "# Plot a sample time series\n",
        "sample_location = wide_df['location'].iloc[0]\n",
        "sample = wide_df[wide_df['location'] == sample_location].sort_values('timestamp')\n",
        "pollutant_for_plot = 'pm25' if 'pm25' in sample.columns else ('pm10' if 'pm10' in sample.columns else sample.select_dtypes('number').columns[-1])\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(sample['timestamp'], sample[pollutant_for_plot], label=pollutant_for_plot.upper())\n",
        "plt.title(f'{pollutant_for_plot.upper()} Trend - {sample_location}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel(f'{pollutant_for_plot.upper()} (standardized units)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. AQI Computation\n",
        "We use US EPA AQI breakpoints.\n",
        "\n",
        "IAQI = (Ihi - Ilo) / (BPhi - BPlo) * (C - BPlo) + Ilo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Breakpoint:\n",
        "    bp_low: float\n",
        "    bp_high: float\n",
        "    i_low: int\n",
        "    i_high: int\n",
        "\n",
        "BREAKPOINTS = {\n",
        "    'pm25': {\n",
        "        'unit': 'ug/m3',\n",
        "        'table': [\n",
        "            Breakpoint(0.0, 12.0, 0, 50),\n",
        "            Breakpoint(12.1, 35.4, 51, 100),\n",
        "            Breakpoint(35.5, 55.4, 101, 150),\n",
        "            Breakpoint(55.5, 150.4, 151, 200),\n",
        "            Breakpoint(150.5, 250.4, 201, 300),\n",
        "            Breakpoint(250.5, 350.4, 301, 400),\n",
        "            Breakpoint(350.5, 500.4, 401, 500),\n",
        "        ],\n",
        "    },\n",
        "    'pm10': {\n",
        "        'unit': 'ug/m3',\n",
        "        'table': [\n",
        "            Breakpoint(0, 54, 0, 50),\n",
        "            Breakpoint(55, 154, 51, 100),\n",
        "            Breakpoint(155, 254, 101, 150),\n",
        "            Breakpoint(255, 354, 151, 200),\n",
        "            Breakpoint(355, 424, 201, 300),\n",
        "            Breakpoint(425, 504, 301, 400),\n",
        "            Breakpoint(505, 604, 401, 500),\n",
        "        ],\n",
        "    },\n",
        "    'o3': {\n",
        "        'unit': 'ppm',\n",
        "        'table': [\n",
        "            Breakpoint(0.000, 0.054, 0, 50),\n",
        "            Breakpoint(0.055, 0.070, 51, 100),\n",
        "            Breakpoint(0.071, 0.085, 101, 150),\n",
        "            Breakpoint(0.086, 0.105, 151, 200),\n",
        "            Breakpoint(0.106, 0.200, 201, 300),\n",
        "            Breakpoint(0.201, 0.604, 301, 500),\n",
        "        ],\n",
        "    },\n",
        "    'co': {\n",
        "        'unit': 'ppm',\n",
        "        'table': [\n",
        "            Breakpoint(0.0, 4.4, 0, 50),\n",
        "            Breakpoint(4.5, 9.4, 51, 100),\n",
        "            Breakpoint(9.5, 12.4, 101, 150),\n",
        "            Breakpoint(12.5, 15.4, 151, 200),\n",
        "            Breakpoint(15.5, 30.4, 201, 300),\n",
        "            Breakpoint(30.5, 40.4, 301, 400),\n",
        "            Breakpoint(40.5, 50.4, 401, 500),\n",
        "        ],\n",
        "    },\n",
        "    'so2': {\n",
        "        'unit': 'ppb',\n",
        "        'table': [\n",
        "            Breakpoint(0, 35, 0, 50),\n",
        "            Breakpoint(36, 75, 51, 100),\n",
        "            Breakpoint(76, 185, 101, 150),\n",
        "            Breakpoint(186, 304, 151, 200),\n",
        "            Breakpoint(305, 604, 201, 300),\n",
        "            Breakpoint(605, 804, 301, 400),\n",
        "            Breakpoint(805, 1004, 401, 500),\n",
        "        ],\n",
        "    },\n",
        "    'no2': {\n",
        "        'unit': 'ppb',\n",
        "        'table': [\n",
        "            Breakpoint(0, 53, 0, 50),\n",
        "            Breakpoint(54, 100, 51, 100),\n",
        "            Breakpoint(101, 360, 101, 150),\n",
        "            Breakpoint(361, 649, 151, 200),\n",
        "            Breakpoint(650, 1249, 201, 300),\n",
        "            Breakpoint(1250, 1649, 301, 400),\n",
        "            Breakpoint(1650, 2049, 401, 500),\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "MOLECULAR_WEIGHTS = {\n",
        "    'o3': 48.00,\n",
        "    'no2': 46.01,\n",
        "    'so2': 64.07,\n",
        "    'co': 28.01,\n",
        "}\n",
        "\n",
        "def ugm3_to_ppm(value_ugm3: float, mw: float) -> float:\n",
        "    return (value_ugm3 * 24.45) / (mw * 1000.0)\n",
        "\n",
        "def normalize_unit(unit):\n",
        "    if unit is None:\n",
        "        return None\n",
        "    unit = str(unit).strip().lower()\n",
        "    unit = unit.replace('\\u00b5', 'u')\n",
        "    unit = unit.replace('ug/m^3', 'ug/m3')\n",
        "    unit = unit.replace('ug/m\\u00b3', 'ug/m3')\n",
        "    unit = unit.replace('mg/m\\u00b3', 'mg/m3')\n",
        "    return unit\n",
        "\n",
        "def convert_to_standard(pollutant, value, unit):\n",
        "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
        "        return None, None\n",
        "    if pollutant not in BREAKPOINTS:\n",
        "        return None, None\n",
        "    unit = normalize_unit(unit)\n",
        "    target_unit = BREAKPOINTS[pollutant]['unit']\n",
        "    if unit == target_unit:\n",
        "        return float(value), target_unit\n",
        "    if pollutant in ('pm25','pm10'):\n",
        "        if unit == 'mg/m3':\n",
        "            return float(value) * 1000.0, target_unit\n",
        "        if unit == 'ug/m3':\n",
        "            return float(value), target_unit\n",
        "        return None, None\n",
        "    mw = MOLECULAR_WEIGHTS.get(pollutant)\n",
        "    if mw is None:\n",
        "        return None, None\n",
        "    if target_unit == 'ppm':\n",
        "        if unit == 'ppb':\n",
        "            return float(value) / 1000.0, target_unit\n",
        "        if unit == 'ug/m3':\n",
        "            return ugm3_to_ppm(float(value), mw), target_unit\n",
        "        if unit == 'mg/m3':\n",
        "            return ugm3_to_ppm(float(value) * 1000.0, mw), target_unit\n",
        "        if unit == 'ppm':\n",
        "            return float(value), target_unit\n",
        "    if target_unit == 'ppb':\n",
        "        if unit == 'ppm':\n",
        "            return float(value) * 1000.0, target_unit\n",
        "        if unit == 'ug/m3':\n",
        "            return ugm3_to_ppm(float(value), mw) * 1000.0, target_unit\n",
        "        if unit == 'mg/m3':\n",
        "            return ugm3_to_ppm(float(value) * 1000.0, mw) * 1000.0, target_unit\n",
        "        if unit == 'ppb':\n",
        "            return float(value), target_unit\n",
        "    return None, None\n",
        "\n",
        "def compute_iaqi(pollutant, concentration, unit=None):\n",
        "    if pollutant not in BREAKPOINTS:\n",
        "        return None\n",
        "    if unit is None:\n",
        "        unit = BREAKPOINTS[pollutant]['unit']\n",
        "    converted, _ = convert_to_standard(pollutant, concentration, unit)\n",
        "    if converted is None:\n",
        "        return None\n",
        "    for bp in BREAKPOINTS[pollutant]['table']:\n",
        "        if bp.bp_low <= converted <= bp.bp_high:\n",
        "            return ((bp.i_high - bp.i_low) / (bp.bp_high - bp.bp_low)) * (converted - bp.bp_low) + bp.i_low\n",
        "    return None\n",
        "\n",
        "def aqi_category(aqi):\n",
        "    if aqi is None or (isinstance(aqi, float) and np.isnan(aqi)):\n",
        "        return None\n",
        "    if aqi <= 50:\n",
        "        return 'Good'\n",
        "    if aqi <= 100:\n",
        "        return 'Moderate'\n",
        "    if aqi <= 150:\n",
        "        return 'Unhealthy for Sensitive Groups'\n",
        "    if aqi <= 200:\n",
        "        return 'Unhealthy'\n",
        "    if aqi <= 300:\n",
        "        return 'Very Unhealthy'\n",
        "    if aqi <= 500:\n",
        "        return 'Hazardous'\n",
        "    return 'Hazardous'\n",
        "\n",
        "def compute_aqi_dataframe(df):\n",
        "    aqi_values = []\n",
        "    for _, row in df.iterrows():\n",
        "        iaqis = []\n",
        "        for p in BREAKPOINTS.keys():\n",
        "            if p in row:\n",
        "                iaqi = compute_iaqi(p, row[p])\n",
        "                if iaqi is not None:\n",
        "                    iaqis.append(iaqi)\n",
        "        aqi_values.append(float(np.max(iaqis)) if iaqis else None)\n",
        "    out = df.copy()\n",
        "    out['aqi'] = aqi_values\n",
        "    out['aqi_category'] = out['aqi'].apply(aqi_category)\n",
        "    return out\n",
        "\n",
        "aqi_df = compute_aqi_dataframe(wide_df)\n",
        "processed_aqi_path = ROOT / 'data' / 'processed_aqi.csv'\n",
        "aqi_df.to_csv(processed_aqi_path, index=False)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "aqi_df['aqi'].dropna().hist(bins=30)\n",
        "plt.title('AQI Distribution')\n",
        "plt.xlabel('AQI')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def add_time_features(df, time_col='timestamp'):\n",
        "    df = df.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "    df['hour'] = df[time_col].dt.hour\n",
        "    df['day_of_week'] = df[time_col].dt.dayofweek\n",
        "    df['month'] = df[time_col].dt.month\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df, group_cols, target_cols, lags=(1,), time_col='timestamp'):\n",
        "    df = df.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "    df = df.sort_values(list(group_cols) + [time_col])\n",
        "    for lag in lags:\n",
        "        for col in target_cols:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "            lag_col = f'{col}_lag{lag}'\n",
        "            df[lag_col] = df.groupby(list(group_cols))[col].shift(lag)\n",
        "    return df\n",
        "\n",
        "pollutant_cols = ['pm25', 'pm10', 'no2', 'o3', 'co', 'so2']\n",
        "features_df = add_time_features(aqi_df, time_col='timestamp')\n",
        "features_df = add_lag_features(features_df, group_cols=['location'], target_cols=pollutant_cols, lags=[1])\n",
        "\n",
        "processed_features_path = ROOT / 'data' / 'processed_features.csv'\n",
        "features_df.to_csv(processed_features_path, index=False)\n",
        "\n",
        "numeric_cols = pollutant_cols + ['aqi', 'hour', 'day_of_week', 'month', 'latitude', 'longitude']\n",
        "numeric_cols = [c for c in numeric_cols if c in features_df.columns]\n",
        "corr = features_df[numeric_cols].corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=45, ha='right')\n",
        "plt.yticks(range(len(numeric_cols)), numeric_cols)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## G. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def train_test_split_time(df, time_col='timestamp', test_size=0.2):\n",
        "    df = df.sort_values(time_col)\n",
        "    split_index = int(len(df) * (1 - test_size))\n",
        "    return df.iloc[:split_index].copy(), df.iloc[split_index:].copy()\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'mae': mean_absolute_error(y_true, y_pred),\n",
        "        'rmse': mean_squared_error(y_true, y_pred, squared=False),\n",
        "        'r2': r2_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "def summarize_metrics(m):\n",
        "    return f\"MAE={m['mae']:.2f}, RMSE={m['rmse']:.2f}, R2={m['r2']:.3f}\"\n",
        "\n",
        "df_model = features_df.dropna(subset=['aqi'])\n",
        "feature_cols = [c for c in (pollutant_cols + ['hour','day_of_week','month','latitude','longitude'] + [f'{p}_lag1' for p in pollutant_cols]) if c in df_model.columns]\n",
        "\n",
        "train_df, test_df = train_test_split_time(df_model, time_col='timestamp', test_size=0.2)\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df['aqi'].values\n",
        "X_test = test_df[feature_cols].values\n",
        "y_test = test_df['aqi'].values\n",
        "\n",
        "baseline_model = Pipeline([('imputer', SimpleImputer(strategy='median')), ('model', LinearRegression())])\n",
        "baseline_model.fit(X_train, y_train)\n",
        "baseline_pred = baseline_model.predict(X_test)\n",
        "baseline_metrics = regression_metrics(y_test, baseline_pred)\n",
        "print('Baseline:', summarize_metrics(baseline_metrics))\n",
        "\n",
        "param_grid = {\n",
        "    'model__n_estimators': [200, 400],\n",
        "    'model__max_depth': [None, 10, 20],\n",
        "    'model__min_samples_split': [2, 5],\n",
        "    'model__min_samples_leaf': [1, 2],\n",
        "}\n",
        "pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('model', RandomForestRegressor(random_state=SEED, n_jobs=-1))])\n",
        "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=TimeSeriesSplit(n_splits=3), scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "tree_pred = best_model.predict(X_test)\n",
        "tree_metrics = regression_metrics(y_test, tree_pred)\n",
        "print('Tree:', summarize_metrics(tree_metrics))\n",
        "print('Best params:', grid.best_params_)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test, tree_pred, alpha=0.4)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual AQI')\n",
        "plt.ylabel('Predicted AQI')\n",
        "plt.title('Predicted vs Actual AQI')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "model = best_model.named_steps['model']\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    importances = model.feature_importances_\n",
        "    order = np.argsort(importances)[::-1]\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(range(len(feature_cols)), importances[order])\n",
        "    plt.xticks(range(len(feature_cols)), np.array(feature_cols)[order], rotation=45, ha='right')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## H. Evaluation & Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_results = test_df.copy()\n",
        "test_results['pred'] = tree_pred\n",
        "test_results['abs_error'] = (test_results['aqi'] - test_results['pred']).abs()\n",
        "\n",
        "test_results['month'] = test_results['timestamp'].dt.month\n",
        "mae_by_month = test_results.groupby('month')['abs_error'].mean()\n",
        "plt.figure(figsize=(8, 4))\n",
        "mae_by_month.plot(kind='bar')\n",
        "plt.title('MAE by Month (Test Set)')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('MAE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "test_results['abs_error'].hist(bins=30)\n",
        "plt.title('Absolute Error Distribution')\n",
        "plt.xlabel('Absolute Error')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I. Conclusions & Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Data limitations: missing pollutants for many timestamps and locations.\n",
        "- Improvements: add weather variables, refine aggregation windows, and apply sensor-level quality filters.\n",
        "- Modeling: try gradient boosting and location-specific models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "report_path = ROOT / 'reports' / 'summary.md'\n",
        "rows_cleaned = len(df_model)\n",
        "time_start = df_model['timestamp'].min().date()\n",
        "time_end = df_model['timestamp'].max().date()\n",
        "pct_missing_aqi = 100 * df_model['aqi'].isna().mean()\n",
        "\n",
        "notes = 'Tree model performs better than baseline; remaining errors likely due to missing context.'\n",
        "\n",
        "report = report_path.read_text()\n",
        "report = report.replace('{{rows_cleaned}}', str(rows_cleaned))\n",
        "report = report.replace('{{time_start}}', str(time_start))\n",
        "report = report.replace('{{time_end}}', str(time_end))\n",
        "report = report.replace('{{aggregation_window}}', 'Daily')\n",
        "report = report.replace('{{pct_missing_aqi}}', '{:.2f}%'.format(pct_missing_aqi))\n",
        "report = report.replace('{{best_model_name}}', 'RandomForestRegressor')\n",
        "report = report.replace('{{mae}}', '{:.2f}'.format(tree_metrics['mae']))\n",
        "report = report.replace('{{rmse}}', '{:.2f}'.format(tree_metrics['rmse']))\n",
        "report = report.replace('{{r2}}', '{:.3f}'.format(tree_metrics['r2']))\n",
        "report = report.replace('{{notes}}', notes)\n",
        "report_path.write_text(report)\n",
        "\n",
        "print(report)\n"
      ]
    }
  ]
}